{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_french_translator_usind_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJCdYcg4yjnJ",
        "colab_type": "text"
      },
      "source": [
        "Collecting the Required Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw-2eh00wx7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "07fadbc2-df94-4399-e618-a4ae8b2c208e"
      },
      "source": [
        "!wget https://www.statmt.org/europarl/v7/fr-en.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-21 06:33:22--  https://www.statmt.org/europarl/v7/fr-en.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 202718517 (193M) [application/x-gzip]\n",
            "Saving to: ‘fr-en.tgz’\n",
            "\n",
            "fr-en.tgz           100%[===================>] 193.33M  4.13MB/s    in 48s     \n",
            "\n",
            "2020-07-21 06:34:12 (4.05 MB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eINURxRyrqW",
        "colab_type": "text"
      },
      "source": [
        "unzipping the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeKnWu8bxl1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "906aa459-3a42-4896-dfde-4a788091abf2"
      },
      "source": [
        "!tar -xvzf /content/fr-en.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "europarl-v7.fr-en.en\n",
            "europarl-v7.fr-en.fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6GAtv-QzjzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "61fff389-f026-485d-a92f-a10526594ce6"
      },
      "source": [
        "#download the non breaking points from the below link and add them to the directory\n",
        " \n",
        " \n",
        "!wget https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P85-Non-Breaking-Prefix-en.zip\n",
        "!unzip P85-Non-Breaking-Prefix-en.zip\n",
        "!wget https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P85-Non-Breaking-Prefix-fr.zip\n",
        "!unzip P85-Non-Breaking-Prefix-fr.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-21 06:34:24--  https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P85-Non-Breaking-Prefix-en.zip\n",
            "Resolving sds-platform-private.s3-us-east-2.amazonaws.com (sds-platform-private.s3-us-east-2.amazonaws.com)... 52.219.101.82\n",
            "Connecting to sds-platform-private.s3-us-east-2.amazonaws.com (sds-platform-private.s3-us-east-2.amazonaws.com)|52.219.101.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1044 (1.0K) [application/zip]\n",
            "Saving to: ‘P85-Non-Breaking-Prefix-en.zip’\n",
            "\n",
            "\r          P85-Non-B   0%[                    ]       0  --.-KB/s               \rP85-Non-Breaking-Pr 100%[===================>]   1.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-21 06:34:25 (54.2 MB/s) - ‘P85-Non-Breaking-Prefix-en.zip’ saved [1044/1044]\n",
            "\n",
            "Archive:  P85-Non-Breaking-Prefix-en.zip\n",
            "  inflating: P85-Non-Breaking-Prefix.en  \n",
            "  inflating: __MACOSX/._P85-Non-Breaking-Prefix.en  \n",
            "--2020-07-21 06:34:30--  https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P85-Non-Breaking-Prefix-fr.zip\n",
            "Resolving sds-platform-private.s3-us-east-2.amazonaws.com (sds-platform-private.s3-us-east-2.amazonaws.com)... 52.219.80.176\n",
            "Connecting to sds-platform-private.s3-us-east-2.amazonaws.com (sds-platform-private.s3-us-east-2.amazonaws.com)|52.219.80.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 960 [application/zip]\n",
            "Saving to: ‘P85-Non-Breaking-Prefix-fr.zip’\n",
            "\n",
            "P85-Non-Breaking-Pr 100%[===================>]     960  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-21 06:34:31 (59.6 MB/s) - ‘P85-Non-Breaking-Prefix-fr.zip’ saved [960/960]\n",
            "\n",
            "Archive:  P85-Non-Breaking-Prefix-fr.zip\n",
            "  inflating: P85-Non-Breaking-Prefix.fr  \n",
            "  inflating: __MACOSX/._P85-Non-Breaking-Prefix.fr  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOTa2IiT1wEX",
        "colab_type": "text"
      },
      "source": [
        "Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz-Je5Gy2auU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl7gyq7b1zFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxQ8BUYM2JU6",
        "colab_type": "text"
      },
      "source": [
        "Loading the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvjP1aKi0BnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/europarl-v7.fr-en.en\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    europarl_en=f.read()\n",
        "with open(\"/content/europarl-v7.fr-en.fr\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    europarl_fr=f.read()\n",
        "with open(\"/content/P85-Non-Breaking-Prefix.en\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_en=f.read()\n",
        "with open(\"/content/P85-Non-Breaking-Prefix.fr\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_fr=f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5-MY80dikzt",
        "colab_type": "text"
      },
      "source": [
        "Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbk_p-d-gozB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the non_breaking_prefixes as clean list of words with a point at the end so it is easier to use.\n",
        " \n",
        "non_breaking_prefix_en=non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en=[' '+pref+ '.' for pref in non_breaking_prefix_en]\n",
        " \n",
        "non_breaking_prefix_fr=non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr=[' '+pref+ '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtsN6kVWkk3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#each word and other symbols that are to keep to be in lowercase and seperated by spaces to tokenize them\n",
        "corpus_en=europarl_en\n",
        "corpus_fr=europarl_fr\n",
        "for prefix in non_breaking_prefix_en:\n",
        "   corpus_en=corpus_en.replace(prefix,prefix+\"###\")\n",
        "corpus_en=re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",\".###\",corpus_en)\n",
        "corpus_en=re.sub(r\"\\.###\",'',corpus_en)\n",
        "corpus_en=re.sub(r\" +\",' ',corpus_en)\n",
        "corpus_en=corpus_en.split(\"\\n\")\n",
        " \n",
        "for prefix in non_breaking_prefix_en:\n",
        "   corpus_fr=corpus_fr.replace(prefix,prefix+\"###\")\n",
        "corpus_fr=re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\",\".###\",corpus_fr)\n",
        "corpus_fr=re.sub(r\"\\.###\",'',corpus_fr)\n",
        "corpus_fr=re.sub(r\" +\",' ',corpus_fr)\n",
        "corpus_fr=corpus_fr.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGJOFTr1rEng",
        "colab_type": "text"
      },
      "source": [
        "TOKENIZING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UJfgS74rGQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en=tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_en,target_vocab_size=2**13)\n",
        " \n",
        "tokenizer_fr=tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_fr,target_vocab_size=2**13)\n",
        " \n",
        "VOCAB_SIZE_EN=tokenizer_en.vocab_size+2\n",
        " \n",
        "VOCAB_SIZE_FR=tokenizer_fr.vocab_size+2\n",
        " \n",
        "inputs=[[VOCAB_SIZE_EN-2]+tokenizer_en.encode(sentence)+[VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        " \n",
        "outputs=[[VOCAB_SIZE_FR-2]+tokenizer_fr.encode(sentence)+[VOCAB_SIZE_FR-1] for sentence in corpus_fr]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp3cr-BVAaYT",
        "colab_type": "text"
      },
      "source": [
        " MAX LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrlDnHfvAe2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH=int(20)\n",
        " \n",
        "idx_to_remove=[count for count, sent in enumerate(inputs) if len(sent)>MAX_LENGTH ]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        " \n",
        "idx_to_remove=[count for count, sent in enumerate(outputs) if len(sent)>MAX_LENGTH ]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwDvgWR5BWsu",
        "colab_type": "text"
      },
      "source": [
        "INPUTS/OUTPUTS CREATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Slu0pV1BZHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#padding the sentences\n",
        "inputs=tf.keras.preprocessing.sequence.pad_sequences(inputs,value=0,padding=\"post\",maxlen=MAX_LENGTH)\n",
        " \n",
        " \n",
        "outputs=tf.keras.preprocessing.sequence.pad_sequences(outputs,value=0,padding=\"post\",maxlen=MAX_LENGTH)\n",
        " \n",
        "#creating datasets\n",
        "BATCH_SIZE=64\n",
        "BUFFER_SIZE=20000 #for shuffling\n",
        "dataset=tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
        "dataset=dataset.cache()\n",
        "dataset=dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset=dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZHBnIZEFDl1",
        "colab_type": "text"
      },
      "source": [
        "MODEL BUILDING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8N-rzeUFICB",
        "colab_type": "text"
      },
      "source": [
        "1.postional encoding\n",
        " \n",
        "Positional encoding formulae:\n",
        " \n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        " \n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR-JwKqsFkak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class PositionalEncoding(layers.Layer):\n",
        " \n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        " \n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX2lggPLHbbF",
        "colab_type": "text"
      },
      "source": [
        "2.Scalar Dot product Attention\n",
        "\n",
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKo9QhNIHmLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYUipWXKIDqV",
        "colab_type": "text"
      },
      "source": [
        "3.Multi Head attention sub layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM3ZbciCIHnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gXmgMaaIMsF",
        "colab_type": "text"
      },
      "source": [
        " 4.Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw9PvDErIQFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuNfTPP3IVMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        " \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwWOMGH_IaL9",
        "colab_type": "text"
      },
      "source": [
        " 5.Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lshz1g0CIdYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJO60RWaIi8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        " \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N9AvW6-Iojr",
        "colab_type": "text"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMxmMv9fIoRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        " \n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YWCQxSZIwo3",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFg0UFJwIzPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " tf.keras.backend.clear_session()\n",
        " \n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        " \n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7lC0LVQI4xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        " \n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        " \n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2s9Oay7I-t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        " \n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        " \n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOkFWTyBJE4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " checkpoint_path = \"./drive/My Drive/tran_ckpt/\"\n",
        " \n",
        "tran_ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        " \n",
        "ckpt_manager = tf.train.CheckpointManager(tran_ckpt, checkpoint_path, max_to_keep=5)\n",
        " \n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    tran_ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJFyZB5RJdUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGuKcixGJnOa",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ4O5jdJJiiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG7sf0gZJ4tz",
        "colab_type": "text"
      },
      "source": [
        "Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO_zCvd3J6dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd0YquSxJ7jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"Hello?\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}